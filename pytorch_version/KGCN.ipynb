{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "from model import KGCN\n",
    "from data_loader import DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument('--dataset', type=str, default='music', help='which dataset to use')\n",
    "# parser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')\n",
    "# parser.add_argument('--n_epochs', type=int, default=100, help='the number of epochs')\n",
    "# parser.add_argument('--neighbor_sample_size', type=int, default=8, help='the number of neighbors to be sampled')\n",
    "# parser.add_argument('--dim', type=int, default=16, help='dimension of user and entity embeddings')\n",
    "# parser.add_argument('--n_iter', type=int, default=1, help='number of iterations when computing entity representation')\n",
    "# parser.add_argument('--batch_size', type=int, default=32, help='batch size')\n",
    "# parser.add_argument('--l2_weight', type=float, default=1e-4, help='weight of l2 regularization')\n",
    "# parser.add_argument('--lr', type=float, default=5e-4, help='learning rate')\n",
    "# parser.add_argument('--ratio', type=float, default=0.6, help='size of training dataset')\n",
    "\n",
    "# args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument('--dataset', type=str, default='ml100k', help='which dataset to use')\n",
    "# parser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')\n",
    "# parser.add_argument('--n_epochs', type=int, default=100, help='the number of epochs')\n",
    "# parser.add_argument('--neighbor_sample_size', type=int, default=8, help='the number of neighbors to be sampled')\n",
    "# parser.add_argument('--dim', type=int, default=16, help='dimension of user and entity embeddings')\n",
    "# parser.add_argument('--n_iter', type=int, default=1, help='number of iterations when computing entity representation')\n",
    "# parser.add_argument('--batch_size', type=int, default=16, help='batch size')\n",
    "# parser.add_argument('--l2_weight', type=float, default=1e-4, help='weight of l2 regularization')\n",
    "# parser.add_argument('--lr', type=float, default=5e-4, help='learning rate')\n",
    "# parser.add_argument('--ratio', type=float, default=0.6, help='size of training dataset')\n",
    "\n",
    "# args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--dataset', type=str, default='bookcrossing', help='which dataset to use')\n",
    "parser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')\n",
    "parser.add_argument('--n_epochs', type=int, default=100, help='the number of epochs')\n",
    "parser.add_argument('--neighbor_sample_size', type=int, default=8, help='the number of neighbors to be sampled')\n",
    "parser.add_argument('--dim', type=int, default=32, help='dimension of user and entity embeddings')\n",
    "parser.add_argument('--n_iter', type=int, default=1, help='number of iterations when computing entity representation')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='batch size')\n",
    "parser.add_argument('--l2_weight', type=float, default=1e-4, help='weight of l2 regularization')\n",
    "parser.add_argument('--lr', type=float, default=5e-4, help='learning rate')\n",
    "parser.add_argument('--ratio', type=float, default=0.6, help='size of training dataset')\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument('--dataset', type=str, default='ml1m', help='which dataset to use')\n",
    "# parser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')\n",
    "# parser.add_argument('--n_epochs', type=int, default=100, help='the number of epochs')\n",
    "# parser.add_argument('--neighbor_sample_size', type=int, default=8, help='the number of neighbors to be sampled')\n",
    "# parser.add_argument('--dim', type=int, default=32, help='dimension of user and entity embeddings')\n",
    "# parser.add_argument('--n_iter', type=int, default=1, help='number of iterations when computing entity representation')\n",
    "# parser.add_argument('--batch_size', type=int, default=2048, help='batch size')\n",
    "# parser.add_argument('--l2_weight', type=float, default=1e-7, help='weight of l2 regularization')\n",
    "# parser.add_argument('--lr', type=float, default=2e-2, help='learning rate')\n",
    "# parser.add_argument('--ratio', type=float, default=0.6, help='size of training dataset')\n",
    "\n",
    "# args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct knowledge graph ... Done\n",
      "Build dataset dataframe ... Done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190</td>\n",
       "      <td>1216</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3317</td>\n",
       "      <td>1824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8686</td>\n",
       "      <td>62938</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15392</td>\n",
       "      <td>61295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15731</td>\n",
       "      <td>55338</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139741</th>\n",
       "      <td>7530</td>\n",
       "      <td>34662</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139742</th>\n",
       "      <td>1855</td>\n",
       "      <td>3786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139743</th>\n",
       "      <td>17081</td>\n",
       "      <td>7056</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139744</th>\n",
       "      <td>3256</td>\n",
       "      <td>73870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139745</th>\n",
       "      <td>6639</td>\n",
       "      <td>8124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139746 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userID  itemID  label\n",
       "0          190    1216      1\n",
       "1         3317    1824      1\n",
       "2         8686   62938      0\n",
       "3        15392   61295      0\n",
       "4        15731   55338      0\n",
       "...        ...     ...    ...\n",
       "139741    7530   34662      0\n",
       "139742    1855    3786      1\n",
       "139743   17081    7056      0\n",
       "139744    3256   73870      0\n",
       "139745    6639    8124      0\n",
       "\n",
       "[139746 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dataset and knowledge graph\n",
    "data_loader = DataLoader(args.dataset)\n",
    "kg = data_loader.load_kg()\n",
    "df_dataset = data_loader.load_dataset()\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class KGCNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id = np.array(self.df.iloc[idx][\"userID\"])\n",
    "        item_id = np.array(self.df.iloc[idx][\"itemID\"])\n",
    "        label = np.array(self.df.iloc[idx][\"label\"], dtype=np.float32)\n",
    "        return user_id, item_id, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_dataset,\n",
    "    df_dataset[\"label\"],\n",
    "    test_size=1 - args.ratio,\n",
    "    shuffle=False,\n",
    "    random_state=999,\n",
    ")\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_test, y_test, test_size=0.5, shuffle=False, random_state=999\n",
    ")\n",
    "train_dataset = KGCNDataset(x_train)\n",
    "val_dataset = KGCNDataset(x_val)\n",
    "test_dataset = KGCNDataset(x_test)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_dataset.__len__())\n",
    "\n",
    "test_loader_epoch = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# prepare network, loss function, optimizer\n",
    "num_user, num_entity, num_relation = data_loader.get_num()\n",
    "user_encoder, entity_encoder, relation_encoder = data_loader.get_encoders()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "net = KGCN(num_user, num_entity, num_relation, kg, args, device).to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2_weight)\n",
    "print(\"device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_version = \"KGCN\"\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch tools loaded\n",
      "start_train: 1669214150.660836\n",
      "[Epoch 1]\n",
      "train_loss:  1.218475737586254\n",
      "val_loss:  1.0021519279915447\n",
      "val_auc:  0.5058752159045528\n",
      "val_f1:  0.5075499807001249\n",
      "--------------------------------\n",
      "Validation loss decreased (inf --> 1.002152).  Saving model ...\n",
      "[Epoch 2]\n",
      "train_loss:  0.8689831809208888\n",
      "val_loss:  0.764110267978825\n",
      "val_auc:  0.508602720424158\n",
      "val_f1:  0.5059092667850424\n",
      "--------------------------------\n",
      "Validation loss decreased (1.002152 --> 0.764110).  Saving model ...\n",
      "[Epoch 3]\n",
      "train_loss:  0.7220862172998306\n",
      "val_loss:  0.7066533203538694\n",
      "val_auc:  0.5115537133666634\n",
      "val_f1:  0.5076688397906157\n",
      "--------------------------------\n",
      "Validation loss decreased (0.764110 --> 0.706653).  Saving model ...\n",
      "[Epoch 4]\n",
      "train_loss:  0.6931373578746144\n",
      "val_loss:  0.697339199721541\n",
      "val_auc:  0.513102886488381\n",
      "val_f1:  0.5070530936239748\n",
      "--------------------------------\n",
      "Validation loss decreased (0.706653 --> 0.697339).  Saving model ...\n",
      "[Epoch 5]\n",
      "train_loss:  0.6869402166637706\n",
      "val_loss:  0.6949078078683653\n",
      "val_auc:  0.5136180734274625\n",
      "val_f1:  0.5081978338435571\n",
      "--------------------------------\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[Epoch 6]\n",
      "train_loss:  0.6843292306563477\n",
      "val_loss:  0.6936136416104286\n",
      "val_auc:  0.5143769452030948\n",
      "val_f1:  0.5065803689348495\n",
      "--------------------------------\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[Epoch 7]\n",
      "train_loss:  0.6828722920177913\n",
      "val_loss:  0.6925705039882224\n",
      "val_auc:  0.5156466350589444\n",
      "val_f1:  0.5070586364159531\n",
      "--------------------------------\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[Epoch 8]\n",
      "train_loss:  0.6821203577536635\n",
      "val_loss:  0.6916314662863675\n",
      "val_auc:  0.5177332604457375\n",
      "val_f1:  0.5062229263293951\n",
      "--------------------------------\n",
      "Validation loss decreased (0.697339 --> 0.691631).  Saving model ...\n",
      "[Epoch 9]\n",
      "train_loss:  0.681736594702049\n",
      "val_loss:  0.6907160576075724\n",
      "val_auc:  0.5200237240831035\n",
      "val_f1:  0.5033194506231478\n",
      "--------------------------------\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[Epoch 10]\n",
      "train_loss:  0.6812653220826532\n",
      "val_loss:  0.6896389129499322\n",
      "val_auc:  0.5249613296082365\n",
      "val_f1:  0.5000063341148318\n",
      "--------------------------------\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[Epoch 11]\n",
      "train_loss:  0.6800392526769783\n",
      "val_loss:  0.6879452445191335\n",
      "val_auc:  0.5397148939402419\n",
      "val_f1:  0.494877532442203\n",
      "--------------------------------\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[Epoch 12]\n",
      "train_loss:  0.6766797837051677\n",
      "val_loss:  0.6841951902598551\n",
      "val_auc:  0.5800298807365072\n",
      "val_f1:  0.5083418317789914\n",
      "--------------------------------\n",
      "Validation loss decreased (0.691631 --> 0.684195).  Saving model ...\n",
      "[Epoch 13]\n",
      "train_loss:  0.6670068160335465\n",
      "val_loss:  0.6728926918822337\n",
      "val_auc:  0.6658418682504058\n",
      "val_f1:  0.564998671260484\n",
      "--------------------------------\n",
      "Validation loss decreased (0.684195 --> 0.672893).  Saving model ...\n",
      "[Epoch 14]\n",
      "train_loss:  0.6379716908559203\n",
      "val_loss:  0.6396098828206868\n",
      "val_auc:  0.7409750530207865\n",
      "val_f1:  0.6455024574409893\n",
      "--------------------------------\n",
      "Validation loss decreased (0.672893 --> 0.639610).  Saving model ...\n",
      "[Epoch 15]\n",
      "train_loss:  0.5765056436790562\n",
      "val_loss:  0.5955112187285402\n",
      "val_auc:  0.7720290540088425\n",
      "val_f1:  0.6852761042381775\n",
      "--------------------------------\n",
      "Validation loss decreased (0.639610 --> 0.595511).  Saving model ...\n",
      "[Epoch 16]\n",
      "train_loss:  0.5048321446449291\n",
      "val_loss:  0.5664030321656841\n",
      "val_auc:  0.7915700867624593\n",
      "val_f1:  0.7123022848558664\n",
      "--------------------------------\n",
      "Validation loss decreased (0.595511 --> 0.566403).  Saving model ...\n",
      "[Epoch 17]\n",
      "train_loss:  0.4432746513474097\n",
      "val_loss:  0.5498426778403591\n",
      "val_auc:  0.8049038099118024\n",
      "val_f1:  0.7278142322114611\n",
      "--------------------------------\n",
      "Validation loss decreased (0.566403 --> 0.549843).  Saving model ...\n",
      "[Epoch 18]\n",
      "train_loss:  0.39466330327275323\n",
      "val_loss:  0.5411773823439803\n",
      "val_auc:  0.8145023644778518\n",
      "val_f1:  0.7411973464316116\n",
      "--------------------------------\n",
      "Validation loss decreased (0.549843 --> 0.541177).  Saving model ...\n",
      "[Epoch 19]\n",
      "train_loss:  0.35769965135060794\n",
      "val_loss:  0.5372820870789219\n",
      "val_auc:  0.8204546584025645\n",
      "val_f1:  0.7499661271658828\n",
      "--------------------------------\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[Epoch 20]\n",
      "train_loss:  0.32993989497445886\n",
      "val_loss:  0.536435702075697\n",
      "val_auc:  0.8240723346806286\n",
      "val_f1:  0.7569614486968939\n",
      "--------------------------------\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[Epoch 21]\n",
      "train_loss:  0.30893172605939934\n",
      "val_loss:  0.5369708788449361\n",
      "val_auc:  0.8260349440139262\n",
      "val_f1:  0.7605934036586351\n",
      "--------------------------------\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[Epoch 22]\n",
      "train_loss:  0.2926005537837471\n",
      "val_loss:  0.5382132055280415\n",
      "val_auc:  0.8272480452628924\n",
      "val_f1:  0.763143558330696\n",
      "--------------------------------\n",
      "EarlyStopping counter: 4 out of 5\n",
      "[Epoch 23]\n",
      "train_loss:  0.2798144046853228\n",
      "val_loss:  0.539874599812782\n",
      "val_auc:  0.8278374988783406\n",
      "val_f1:  0.764657974459635\n",
      "--------------------------------\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "end_train: 1669215128.4740083\n",
      "end_train - start_train: 977.8131723403931\n"
     ]
    }
   ],
   "source": [
    "from pytorchtools import EarlyStopping\n",
    "# add early stopping\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True, path=f'./checkpoint/{name_version}_{args.dataset}.pt', delta=0.005)\n",
    "# early_stopping = EarlyStopping(patience=patience, verbose=True, path=f'./checkpoint/{name_version}_{args.dataset}.pt', delta=0.01)\n",
    "\n",
    "# train\n",
    "loss_list = []\n",
    "val_loss_list = []\n",
    "auc_score_list = []\n",
    "f1_score_list = []\n",
    "\n",
    "import time\n",
    "\n",
    "start_train = time.time()\n",
    "print(\"start_train:\", start_train)\n",
    "\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (user_ids, item_ids, labels) in enumerate(train_loader):\n",
    "        user_ids, item_ids, labels = (\n",
    "            user_ids.to(device),\n",
    "            item_ids.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(user_ids, item_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # print train loss per every epoch\n",
    "    print(\"[Epoch {}]\".format(epoch + 1))\n",
    "    print(\"train_loss: \".format(epoch + 1), running_loss / len(train_loader))\n",
    "    loss_list.append(running_loss / len(train_loader))\n",
    "\n",
    "    # evaluate per every epoch\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        total_roc = 0\n",
    "        total_f1 = 0\n",
    "        for user_ids, item_ids, labels in val_loader:\n",
    "            user_ids, item_ids, labels = (\n",
    "                user_ids.to(device),\n",
    "                item_ids.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            outputs = net(user_ids, item_ids)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            outputs = outputs.cpu().detach().numpy()\n",
    "            labels = labels.cpu().detach().numpy()\n",
    "            total_roc += roc_auc_score(labels, outputs)\n",
    "            outputs = np.where(outputs >= 0.5, 1, 0)\n",
    "            total_f1 += f1_score(labels, outputs)\n",
    "\n",
    "        print(\"val_loss: \".format(epoch + 1), val_loss / len(val_loader))\n",
    "        print(\"val_auc: \".format(epoch + 1), total_roc / len(val_loader))\n",
    "        print(\"val_f1: \".format(epoch + 1), total_f1 / len(val_loader))\n",
    "        print(\"--------------------------------\")\n",
    "        val_loss_list.append(val_loss / len(val_loader))\n",
    "        auc_score_list.append(total_roc / len(val_loader))\n",
    "        f1_score_list.append(total_f1 / len(val_loader))\n",
    "\n",
    "    # early stopping\n",
    "    early_stopping(val_loss / len(val_loader), net)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "end_train = time.time()\n",
    "print(\"end_train:\", end_train)\n",
    "print(\"end_train - start_train:\", end_train - start_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: [0.38348708 0.38004613 0.56233376 ... 0.44893408 0.3101708  0.373608  ]\n",
      "labels: [0. 0. 0. ... 0. 0. 0.]\n",
      "test_auc:  0.8175006227426221\n",
      "test_f1:  0.7484843175048187\n"
     ]
    }
   ],
   "source": [
    "# load the last checkpoint with the best model\n",
    "net = KGCN(num_user, num_entity, num_relation, kg, args, device).to(device)\n",
    "net.load_state_dict(torch.load(f'./checkpoint/{name_version}_{args.dataset}.pt'))\n",
    "\n",
    "# test\n",
    "with torch.no_grad():\n",
    "    total_roc = 0\n",
    "    total_f1 = 0\n",
    "    for user_ids, item_ids, labels in test_loader:\n",
    "        user_ids, item_ids, labels = (\n",
    "            user_ids.to(device),\n",
    "            item_ids.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "        outputs = net(user_ids, item_ids)\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        print(\"outputs:\", outputs)\n",
    "        print(\"labels:\", labels)\n",
    "        total_roc += roc_auc_score(labels, outputs)\n",
    "        outputs = np.where(outputs >= 0.5, 1, 0)\n",
    "        total_f1 += f1_score(labels, outputs)\n",
    "\n",
    "    print(\"test_auc: \", total_roc / len(test_loader))\n",
    "    print(\"test_f1: \", total_f1 / len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc:  0.8177852042568738\n",
      "test_f1:  0.7483137543077604\n"
     ]
    }
   ],
   "source": [
    "# load the last checkpoint with the best model\n",
    "net = KGCN(num_user, num_entity, num_relation, kg, args, device).to(device)\n",
    "net.load_state_dict(torch.load(f'./checkpoint/{name_version}_{args.dataset}.pt'))\n",
    "\n",
    "# test\n",
    "with torch.no_grad():\n",
    "    total_roc = 0\n",
    "    total_f1 = 0\n",
    "    for user_ids, item_ids, labels in test_loader_epoch:\n",
    "        user_ids, item_ids, labels = (\n",
    "            user_ids.to(device),\n",
    "            item_ids.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "        outputs = net(user_ids, item_ids)\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        # print(\"outputs:\", outputs)\n",
    "        # print(\"labels:\", labels)\n",
    "        total_roc += roc_auc_score(labels, outputs)\n",
    "        outputs = np.where(outputs >= 0.5, 1, 0)\n",
    "        total_f1 += f1_score(labels, outputs)\n",
    "\n",
    "    print(\"test_auc: \", total_roc / len(test_loader_epoch))\n",
    "    print(\"test_f1: \", total_f1 / len(test_loader_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('vbd_ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "26e5dfdc880656ff8597435c7ab15c7a8b4608cd1fde4db03057ac4fc671b31a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
